04/30/14
First basic implementation of an automaton. Question: which alphabet? We will here consider the integers. NO UPPER BOUND hypothesis must be made!
Mu and Rho are (hash)maps.
There seems to be a link with CNF and SAT solving problem; take a look later.
For now, the transition set is seen as sparse: a matrix representation is therefore useless. We also maximise reading access times: the transition system is seen as a hashset (key: state) of a hashset (key: label) of a list of states.
Each automaton object is seen as an automaton model; the object is immutable during any computation.
First write deterministic algorithm.
New question: how does one perform a quick check of whether there is a register that contains the current symbol? For now, we'll do it naïvely: check all the registers. Later on, an optimisation is to keep a record of the current registers' content in form of a (hash)set.
This algorithm is/could be linear with the number of symbols to test. The number of registers and states does not impact the computation time.

Questions:
“A Turing machine can simulate membership for NFA in NLOG”: how?
Computational complexity of MEMD.
Why use reduction while it's straightforward?

See email dated today: ignore these questions, authors made some unclear assertions.

05/01/14
[coding nondeterministic membership]
Try:
A basic BFS/other search algorithm. An interesting heuristic should be found.
A different vision over the problem:

A RA can be seen as an NFA over a finite word w over a finite alphabet E: w is composed of the register indices where symbols are progressively stored (or not); in the end a word is exactly a string of transition labels, as shown in the representation of a RA.

[reflexion on emptiness]
If there is a word in the language defined by an automaton A, then the following algorithm will return such a word:
BFS/A* search from initial state to final state. Not to be confused with the search STATE.
If the language is empty, this search will not terminate, which makes the algorithm/the method useless.

FMA (Kaminski) has some mistakes, i.e. p 334 tau values do not have to be distinct (“abaaba” passes membership test).

05/02/14
Finished nondeterministic membership problem with a first naïve solution: just LDFTS. A next step would be to find a nice heuristic.

Could it be possible to divide an automaton into two subsets, deterministic states and nondeterministic ones, and then see the problem as a “chance” game? Or a game with contingency plans? MINIMAX?

05/03/14
Empirical emptiness problem: PROPOSITION: it is only necessary to test the automaton with the following subset of the alphabet:
{elements in the initial registers} U {other distinct elements}
the total set size being the number of registers + 1.

WHY: we always need the possibility of filling up all the registers and having new elements coming in, which is why there need to be at least NR+1 elements in the alphabet subset.
The automaton can only remember seeing NR elements, so if there is always a new element, it will be seen as “new”.

05/04/14
The first version of a naïve empirical emptiness check works. It probably does way too much work: when all words of size N are refused, words of size N+1 are checked, and the work done on size N is forgotten and recomputed (very intensive).
In each case it works perfectly, and for example for Example 3 (automaton recognising words with double repetition, ie two different symbols are repeated): [1,1,0,0] is returned.

Idea: create a “generative” emptiness check, with the previous intuition; the decision algorithm would actually be complete given the previous PROPOSITION: the state space is big but not infinite.
The decision algorithm would be based on a BFGS/A*G algorithm, where the G factor is very important: we are not interested in cycles. We can see the problem as a search problem from an initial state to whatever final state we reach first. A* could use a heuristic based on the physical distance of the state to a final state (to be precomputed).
WARNING: there might be unvoluntary cycles introduced if we go to the same state, with the same registers assigned, but with different values. Since all assigned values are distinct, there might be some symmetry (isomorphism) in this CSP that could be avoided.

05/05/14
Generative emptiness seems to work OK. Some mistakes have been made regarding permutations of register values (see previous WARNING).

-> Meeting with supervisor; TODO:
- Deterministic membership is straightforward, agreed.
- Nondeterministic membership can be optimised by using an adapted version of BFTS: Breadth-First-Local-Graph-Search. LDFTS performs less good in a diamond-chain-shaped automaton's case: BFLGS will be linear in all cases, LDFTS will be exponential as usual. Try to find an example where LDFTS performs better; idea: choose a tree that has a lot of branches with a deep-found solution. This only works better if the word is part of the language.
- Nondeterministic can also be solved by assigning hashmaps to each physical state, where a value (label?) points towards a set of configurations. To be explored and implemented.
- Emptiness decision problem: the equality check should be reviewed as explained above.
- Read the paper about emptiness, the proof is probably very similar to what's going on in the chosen algorithm.
- Also try out a heuristic for emptiness checking (A*). This might speed up cases where the language is nonempty.
- Explore language inclusion
- All the previous algorithms should be tested on useful examples of automata. Random generation can be inspired from random graph generation. Then a good idea is to start with a small sticky manual example, and try to generalise it and automatise its generation. The diamond-chain-shaped automata are a good example of what kind of automaton can be automatically generated.

Example 4 is a basic example where LDFTS is disastrous in case the word is not in the language.

It seems that BFLGS is indeed useful in the case of a diamond-chain-shaped automaton, but how useful is it in average? And compared to LDFTS?
STRATEGY: 
1) Get tests up and running: 
	a. Find interesting automata for testing
	b. Improve the results container to get more interesting results:
		- Number of nodes expanded
		- Frontier size
		- Operations performed
2) Implement different algorithms for nondeterministic membership
3) Compare on large automata

So there we go:
1) Random automata seem OK, but we have to find out how to choose the parameters that go with them. Quick cleanup of RAGenerator's structure.

Finished Diamond-chain-shaped (DCS) automata.

05/06/14
Now improving test infrastructure.

05/08/14
Test infrastructure has been improved a lot. Some work still needs to be done (polymorphism of algorithms and search algorithms for instance).
Diamond-chain-shaped automata seem to give the expected result: DFS is explosive (exponential increase) while SB-BFS is linear. Tests made with listed membership test infrastructure.

Some questions:
- How useful would it be to test all the algorithms?
- Why is the DCS automaton's instance interesting?
- Try to find a nice example where BFLGS loses all its efficiency.

* The point is that DFS can be -asymptotically- worse than SB-BFS (see example 4).

SB-BFS can be worse than DFS but not asymptotically. 
Proof:

We're going to show that SB-BFS has, in the worst case, always the same asymptotic performance as DFS. The case where membership fails is straightforward: DFS and SB-BFS expand all the nodes, and because of pruning, SB-BFS does always better. We're looking for a case where DFS decides faster, and that's only possible if membership succeeds.

Reformulation:
Let A be an automaton, with registers R and state set Q. Let (wn) be an infinite word list such that DFS finds a path in O(f(n)) nodes expanded for wn in A, and |wn| = n (i.e. the path is of length n). We suppose that, on this list, SB-BFS is worse in the number of nodes expanded. Let's show that SB-BFS's performance is in O(f(n)).

Let S be a frontier at some point in SB-BFS's exploration. The search states in this frontier are distinguished by three criteria:
- The sublist of letters in the word left to explore
- The values in the registers
- The name of the current physical state

As it happens:
- A frontier in BFS symbolises a circle in a state space where the distance is the number of nodes in the path to go from one space to the other; in other words, for every search state in S, the sublist of letters to explore is the same.
- Let A be the set of letters in the initial word, and |A| its size. Then there are D = (|A|+1)^|R| possibilities of register assignments.
- There are |Q| possible physical states.
As a consequence, the maximum number of search states in S is D*|Q|.

SB-BFS finds the solution in n frontier swaps. Therefore it must expand in the worst case D*|Q|*n nodes. But:
- DFS has a better performance in this example. Let g(n) be the number of nodes DFS expands in search for the final state. Then g(n) <= D*|Q|*n.
- DFS must at least expand n nodes. Therefore

		n <= g(n) <= D*|Q|*n

and two results are inferred:
- g(m) = O(m) (i.e. a candidate for f is n → n)
- since D*|Q|*m = O(m), SB-BFS has the same asymptotic performance as DFS.

Therefore, in case DFS has a better performance than SB-BFS, even then SB-BFS does not have a worse asymptotic performance.

A case where SB-BFS is worse than DFS: example5.fma. See test21.csv. This example is a fork between example 4 and another useless cyclic automaton. BFS's frontier will always be bigger, and therefore in DFS's optimal case, DFS is faster. Furthermore, there is just a factor 3 difference.

DFS is always worse than SB-BFS when membership fails.

05/10/14

What happens if DFS had a priority-set? Direct consequence: its frontier cannot have repetitions.

This would have no consequence at all: almost every node in DFS's frontier respects the following rules:
- either the nodes have a unique word-to-explore length left
- or, if that's not the case, they are different physical states.
This would only be useful in case several identical transitions go from the same node to the same node.

The idea to implement states with hashsets seems already used: for the moment, a configuration can input a label to output a configuration. Ask again for what the idea was.

The membership problem with heuristics: A* to solve membership. Interesting when there are many choices of paths; A* would use a priority-set. But which heuristic?
→ [[Physical distance from goals]]: constraint relaxation on the label of the transitions. Two problems arise:
	* There are often several final states; this implies several distances, while a heuristic is real-valued
	* Cycles introduce more different distances, and with this distances can be in infinite numbers

IDEA: pre-process the automaton and attribute a score map to each physical state, ie for state s a function f_s such that:
	f_s : n → f_s(n),
where n is the number of letters left to process in the word, and f_s(n) is the number of physical paths that exist from that node to a final node.

What is the value of f_f(0) where state f is final? -1 could be used for unset, -2 for final states, 0 for states with no path to a final state, and >0 for regular values.

Note that this could lead to a lighter version of the automaton, with all terminal non-final states removed...

f_s is difficult to find. The best way would be to have a real function making a computation using + and * operations, based on cycles detection. Is that possible? If not we'll have to find alternatives:
	* A first solution consists in computing f_s(n) naïvely on the spot. A* expands a node, a few new configurations arrive, what is their f_s score? This results in a basic graph search for a path of determined length to final nodes (BFS: count all the final nodes at the bottom of the full search tree). This is very computationnally expensive and doesn't seem to add any performance.
	* A second solution could be to compute f_s(n) on the spot, but by using peer f_s values: let q1 and q2 be two adjacent states, and suppose m transitions exist from q1 to q2. Then:
	f_q1_q2(n) = m * f_q2(n-1) if f_q2(n-1) > 0 else 0
and f_q1 = sum(f_q1_q, q adjacent to q1).


(NB: suppose adjacent node s has p paths to final state. If we have one transition to s, that means still p paths. If we have two transitions, that means 2p paths, because for each path we can choose either the first or the second transition twice the number of possibilities).)

Each time an f_s is computed, its value is stored in an array (and not a hashset!) contained within the physical set, and can then be reused later. If adjacent values exist, this method is not too expensive. It is however impractical if, say, the aim is to test only one word of length 1,000, 10,000, …
	* A third solution is to compute f_s for all states s for all lengths n < N before actually starting any membership tests. This is a linear operation in N. Then the automaton could be pruned from useless states as pointed out above.

All in all these methods will take a lot of memory (one million states with one million-length words is one terabyte of necessary memory and is not computable; ten thousand states with word length a thousand is 10 million bytes (10M) which is still acceptable), and the initial computation process (if present) might take some time.

In the code repository, f will named “hscore”.

→ A heuristic based on labels. Could make some kind of density in the label “1”, which could advise on a certain direction to take...

05/12/14
Meeting with supervisor.
------------------------

- Previous proof was OK but it's possible to find better asymptotically for DFS if the automaton varies as well. The idea is an automaton with n branches for the initial state, and it's actually a tree of depth n with all leaf states final: DFS finds the solution in n, and BFS in 3^n for example.

- MORE on paper

Implementing heuristic: physical distance.

The Hscore value can be computed faster if all terminal states (without outgoing transitions) have their values set to 0 and are then ignored. 

Practical question: would it be better to have a set instead of a list in the transition hashmap? A list allows to reach the same state twice or more from the same state with the same label, which is useless...

Think about a speedup algorithm: reduce automata. Remove terminal states, double transitions, and think about more.

It seems that the Hscore idea has a problem: the numbers are too large. Think about it: a one-state double recursion implies 2^1000 paths for a 1000-letter long word. It is impossible to store this number. However, we might want to store the saturated value just for the principle of making the point: “here are a lot of possibilities!”.

Small optimisation: PrioritySet will not accept states with score 0.

Test infrastructure needs to be improved further to make decision algorithms additions easier.

05/13/14
Question: is there a way, given the minimal alphabet subset, to transform an FMA into an NFA?

Implemented better test infrastructure, algorithms are now objects.

Debugging Best-First search.

05/14/14
Best-First search works.

In which case(s) does it work better than BFLGS? For the handwritten examples, it expands less nodes, but is more costly time-wise.

It obviously works better as the automaton's search tree grows “wider”; try to find an example for this.

Suppose the automaton is a tree with several branches from the root but then only 1-width paths to final states. In that case Best-First search will entirely behave like BFLGS: the heuristic will push it to explore all the root's branches step by step. DFS would remain -asymptotically- better.

→ Best-First search has to be improved! There must be a proximity factor included. 

This proximity factor is easy to find: we know the distance from our goal. A suggestion for including this proximity factor is a lexicographical comparison in form of (distance, number of paths > 0).

This implies modifying the frontier and create an A*ish version of best-first. Done.

Let's now implement a new generator that, given n, generates a tree with n root signle branches terminated by final states.

Most of the examples only have one or two registers; especially for the ones with one register, this kind of defeats the purpose of register automata: where's the difference with NFA?

Best-First is dangerous because elements are placed randomly in the queue if they have the same heuristic. The frontier at that point is no longer a queue nor a stack. Rather than forcing this behaviour it seems more legitimate to actually abaondon the concept.

05/16/14
Reading, and trying to answer the following question: do FMAs include NFAs, or vice-versa, or not at all?

FMAs include NFAs in a quite obvious manner (initialise registers of size of finite alphabet and deactivate all rhos).
05/18/14
For emptiness, there is an easy heuristic to apply as well: physical distance from any target. [TODO]

Register content checking is very slow: linear check in the number of registers. Optimisation to get rid of this linear factor: instead of storing (regNumber → regSymbol), we store (regSymbol → regNumber, or <0 if not assigned). This has the same memory consumption if the alphabet is encoded intelligently (R elements and therefore a very small array).

05/19/14
Meeting with Radu:
- HEURISTICS
Need proof that it actually works well. Targetted word size, if automaton size doesn't change, is around a million.
Try to find a workaround for the integer limit problem by using logarithms (which conserve order).
- INCLUSION
Read the paper about the inclusion proof: with more than two registers “the problem is undecidable”.
Test inclusion up to a certain length by generating paths similarly to emptiness, and by testing membership. Test on both example automata.
- JAVA TRACES
Useful to test properties. Things to do:
* Implement an RA for the hasNext property
* Collect data and (filter) translate into a word that is understandable by the automaton
* Test with different algorithms.
* Implement more properties (Collections, HttpResponse)

Also two papers to read about related subjects: Paths through graphs, and testing algorithms.

Will start by gathering test data from Java programs.

05/20/14
Succeeded in creating a JDI implementation to monitor a program's acitvity. Now possible to translate into intelligent integers, for property checking.

05/21/14
1. Write translator
2. Write automaton example
3. Extend test infrastructure for 1 word only
4. Test with different program examples

Tests made on single-iterator example: single-iterator.tr

All done. In case membership fails, even for the has-next property, LDFTS is unable to terminate in time. A* has terrible performance as well; the only one more or less reasonable is BFLGS.

05/22/14
In case of membership failure (which is what we are trying to prove), it's normal A* fails miserably: it will have the same behaviour as LDFTS.

Trying with more complex example programs for HNP: Multiple Random Iterator Generation results in ultra large traces, and these take ages to process.

A less naïve version of the translation (only relevant events) allow to 

Weirdly enough, the number of expanded nodes increases linearly, and the time squarely.

05/26/14
Email Runtime Verification owner: to get properties. [DONE]
DaCapo: traces from other projects, run with custom debugger.
More properties.
Paper: API usage patterns + Radu's paper
Translation including garbage collection preprocessing: grabage state.
Use profiler to accelerate

Bloom Filters: fast membership checking in a set.

Sets in registers
SAT subsuming.

---

No need for word comparison when computing hashcodes for BFLGS: for each state in a frontier, all word-to-explore lengths are the same.

Action plan:
------------
1. Go over BFLGS for some low-level optimisations and run the same tests again for comparison. Use profiler if necessary.
2.a Look at return values for traces; modify the hasNext property test.
2.b Implement garbage collection given this: extend the automaton.
2. Implement the properties on Wikipedia and test them.
3. Ask for Radu's paper.
4.a Look at assigning hash maps to each state in order not to compute “expand”.
4.b Explore new ways of automaton exploration: Bloom filters? belief states? sets in registers? Can be useful for emptiness.
5. Look at Runtime Verification papers: can their techniques be used with RA? How do logic languages behave regarding RA?

05/27/14
Idea to be explored later: set-based BFS puts word-to-explore lengths on the same level. What if a search algorithm did the same but with the configuration's “current state” or the configuration's “register contents” (up to isomorphism)? To be tried out...

Go for action plan:
1. Going through BFLGS for optimisations.
Will clone search state and search node code for direct optimisations.

Stripped BFLGS (now opti BFLGS) from redundant hashcode checking: performance is skyrocketting! Will acquire more test data because currently insufficient.

Maximum test data length is now 30,000 symbols. Now 50,000! But the JVM seems to be doing plenty of stuff during the tests, therefore killing the performance for some words.

VisualVM says the most used methods are “decide” and “expand”, which is not a surprise. Trying to optimise now...

To be tested: replace the array list by a linked list for the left-over word. Try generating all the search states and choose from them, rather than creating new objects each time. Try collapsing search states and nodes.

Tried right away: change decision function type from “int[]” to “list<int>” where the implementation of the list is not important.

Analysing the performance increase pooling could have on this memory issue: Garbage Collection seems to happen at the wrong moment, killing measures. How to circumvent this by reusing objects? 

05/28/14
Article below claims this has no use, which makes sense. Maybe the question is rather, how to benchmark with Java?

THE ERROR: two-fold:
- by remembering the path of node parents, all nodes are kept! (Inform Peter Jeavons)
- by making sublists of sublists, all the sublists are kept in memory, the whole tree!

Taking out the sublists is a 10% gain on time.
Taking out paths allows to handle 100,000 symbols in under 10s, probably because GC issues are no longer.

05/29/14
Tiny optimisation: when considering checking a property, to know whether a search state is final, no need to check the word size: if it is possible to reach a final state with the current trace, the property check fails!

2.a Replaced all entry events by exit events, in order to have their return value has well.

Adapting translator to incorporate return values from hasNext: done. Now rewriting automaton.

05/30/14
There is a performance loss because of the paradigm: “Any symbol should either be present or stored”. The model should be extended with the possibility to ignore the next symbol.
